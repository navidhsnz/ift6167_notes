This lecture was on the #paper: [[Chinchilla.pdf]]
prerequisite: [[attention is all you need.pdf]]
***
![[chinchilla image.png|200]]
## Other Resources:

[Youtube video explaining the paper](https://www.youtube.com/watch?v=PZXN7jm9IC0)
![](https://www.youtube.com/watch?v=PZXN7jm9IC0)

Blog post: [Scaling Laws for LLMs: From GPT-3 to o3](https://cameronrwolfe.substack.com/p/llm-scaling-laws)




