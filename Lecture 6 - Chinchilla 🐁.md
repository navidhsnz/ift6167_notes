***
This lecture was on the #paper: [[Chinchilla.pdf]]

![[chinchilla image.png|200]]

[Youtube video explaining the paper](https://www.youtube.com/watch?v=PZXN7jm9IC0)

![](https://www.youtube.com/watch?v=PZXN7jm9IC0)


Very good blog post (must-read): [Scaling Laws for LLMs: From GPT-3 to o3](https://cameronrwolfe.substack.com/p/llm-scaling-laws)
PDF version of the blog post above in case it disappears on internet: [[Scaling Laws for LLMs_ From GPT-3 to o3.pdf]]

#paper: Paper after Chincilla: [[beyond chinchilla optimal.pdf]]


Test time scaling:  [youtube video](https://www.youtube.com/watch?v=6PEJ96k1kiw)
![](https://www.youtube.com/watch?v=6PEJ96k1kiw)


Blog post on post training: [[A recipe for frontier model post-training.pdf]]
***
## History ðŸ“œ

#paper Large autoregressive transformers original paper [[attention is all you need.pdf]]
#Blogpost [[Decoder-Only Transformers_ The Workhorse of Generative LLMs.pdf]]
### GPT lineage:
- GPT1: #paper [[GPT1 - language_understanding_paper.pdf]] 
- GPT2: #paper [Language Models are Unsupervised Multitask Learners](GPT2.pdf)
- GPT3: #paper[[Language models are few shot learner.pdf]]
- GPT4: #paper[[gpt4.pdf]]


