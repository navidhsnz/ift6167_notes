***
This lecture was on the #paper: [[Chinchilla.pdf]]

![[chinchilla image.png|200]]
## Other Resources:

[Youtube video explaining the paper](https://www.youtube.com/watch?v=PZXN7jm9IC0)

![](https://www.youtube.com/watch?v=PZXN7jm9IC0)


Blog post: [Scaling Laws for LLMs: From GPT-3 to o3](https://cameronrwolfe.substack.com/p/llm-scaling-laws)
PDF version of the blog post above in case it disappears on internet: [[Scaling Laws for LLMs_ From GPT-3 to o3.pdf]]


***
## History ðŸ“œ

#paper: large autoregressive transformers original paper [[attention is all you need.pdf]]

#paper: GPT1 paper: [[GPT1 - language_understanding_paper.pdf]] 
Annotated version by Shreyansh: [[GPT1_annotated.pdf]]



